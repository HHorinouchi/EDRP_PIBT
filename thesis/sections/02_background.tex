\section{背景となる研究}\label{sec-background}
本章では, 本研究の基礎となるMulti-Agent Path Finding (MAPF) 問題, およびその応用問題であるMulti-Agent Pickup and Delivery (MAPD) 問題について概説する. 特に, 本研究ではドローン配送などの実世界応用を念頭に置き, MAPFを物理的制約を考慮して拡張したモデルであるDrone Routing Problem (DRP) \cite{Kaji2025} の定義に基づくグラフベースのモデルを採用する. 続いて, 本研究で採用する分散型アルゴリズムであるPIBTについて述べ, その課題を明らかにする. 最後に, PIBTにおけるパラメータ最適化手法として採用する進化戦略 (Evolution Strategy) について解説する. 

% =============================================
% 2.1 MAPFとDRP
% =============================================
\subsection{MAPFからの拡張モデル: DRP}
Multi-Agent Path Finding (MAPF) 問題は, 複数のエージェントが現在位置から目的地まで, 互いに衝突することのない経路を計画する問題である. すべてのエージェントが目的地に到達するまでのコスト(総移動時間や移動距離の和など)を最小化することを目標とする. \par

従来の基本的なMAPF問題は, 図\ref{fig:two_images}(a)に示すようなグリッド環境上で定義され, エージェントは離散的なセル間を移動するモデルが一般的であった. しかしながら, ドローン配送のような実空間での運用を考慮した場合, 移動空間は必ずしも均質なグリッド構造には限定されない. Kajiらは, このような問題をDrone Routing Problem (DRP)\cite{Kaji2025}として定義し, ノード間の物理的な距離やエージェントの連続的な位置を考慮した2次元グラフマップ上での経路計画問題として定式化した. 
そこで本研究では, 環境をグリッドではなく図\ref{fig:two_images}(b)のような一般的な無向グラフとして定義し, 従来のMAPF問題を物理的な移動制約および連続空間での衝突回避を考慮して拡張したDRPモデルを取り扱う. 

\begin{figure}[htbp]
  \centering
  % --- 左の画像 ---
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    % 画像ファイルがある場合
    \includegraphics[width=\linewidth]{images/grid_map.png}
    
    % 手動でキャプションを書く
    \centerline{(a) グリッド環境のマップ}
  \end{subfigure}
  \hfill % 画像間のスペース調整
  % --- 右の画像 ---
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/graph_map.png}
    
    % 手動でキャプションを書く
    \centerline{(b) 無向グラフ環境のマップ (DRPモデル)}
  \end{subfigure}
  % 全体のキャプション
  \caption{2つの環境モデルの比較}
  \label{fig:two_images} % ← 親のラベルは有効
\end{figure}

\subsubsection{問題の定義}
本研究では, Kajiら\cite{Kaji2025}の定義に基づき, DRPを物理的制約を持つグラフ上でのMAPF問題として定式化する. 
問題はタプル $\mathcal{M} = \langle G,  \mathcal{A},  \mathcal{T},  \Sigma,  \mathcal{C},  \mathcal{W} \rangle$ によって定義される. 

\begin{itemize}
    \item $G = (V,  E)$ は無向グラフであり, $V$ はノード集合, $E \subseteq V \times V$ はエッジ集合を表す. 
    \item $\mathcal{A} = \{a_1,  a_2,  \dots,  a_m\}$ は $m$ 体のエージェント（ドローン）の集合である. 
    \item $\mathcal{T} = \{0,  1,  2,  \dots \}$ は離散化された時刻（タイムステップ）の集合である. 
    \item $\Sigma = \{(s_i,  g_i) \mid s_i,  g_i \in V,  a_i \in \mathcal{A}\}$ は各エージェントの始点ノード $s_i$ と終点ノード $g_i$ のペアの集合である. 
    \item $\mathcal{C}: V \to \mathbb{R}^2$ は, 各ノード $v \in V$ を2次元ユークリッド空間上の座標に対応付ける関数である. 
    \item $\mathcal{W}: E \to \mathbb{R}^+$ は各エッジ $(u,  v) \in E$ の物理的な長さ（距離）を定義する重み関数であり, $\mathcal{W}(u,  v) = \lVert \mathcal{C}(u) - \mathcal{C}(v) \rVert$ で与えられる. 
\end{itemize}

さらに, 各エージェント $a_i$ には以下の物理パラメータが与えられる. 
\begin{itemize}
    \item $v_{\text{max}} \in \mathbb{R}^+$: エージェントが1タイムステップに進むことができる最大物理距離（速度）. 
    \item $r_{\text{safe}} \in \mathbb{R}^+$: 衝突回避のために維持すべき安全半径（エージェント間の最小距離は $2r_{\text{safe}}$）. 
\end{itemize}

\subsubsection{状態と行動モデル}
各タイムステップ $t \in \mathcal{T}$ におけるエージェント $a_i$ の状態は, 物理的な位置座標 $\bm{p}_i(t) \in \mathbb{R}^2$ および, グラフ上の所在（直前のノード $u$, 向かっているノード $v$）によって管理される. 
初期状態は $\bm{p}_i(0) = \mathcal{C}(s_i)$ である. 

エージェントは各ステップにおいて, 隣接ノードへの移動または待機を選択する. 
行動 $act_{i, t}$ がノード $v_{next} \in V$ への移動である場合, エージェントは現在の位置から $v_{next}$ に向かって最大 $v_{\text{max}}$ だけ進む. 
位置座標 $\bm{p}_i(t)$ の更新は以下のように定義される. 

\begin{equation}
    \bm{p}_i(t+1) = \bm{p}_i(t) + \delta \cdot \bm{u}
\end{equation}

ここで, $\bm{u}$ は進行方向の単位ベクトル, $\delta$ は実際の移動距離を表す. 現在のターゲットノードを $v_{next}$ とすると, これらは以下のように計算される. 

\begin{align}
    \bm{d} &= \mathcal{C}(v_{next}) - \bm{p}_i(t) \\
    \bm{u} &= \frac{\bm{d}}{\lVert \bm{d} \rVert} \\
    \delta &= \min(v_{\text{max}},  \lVert \bm{d} \rVert)
\end{align}

すなわち, DRPにおけるエージェントは, 離散的な時間ステップごとにグラフのエッジ上を連続的に遷移する. 目的地までの距離が $v_{\text{max}}$ 未満の場合は, オーバーシュートすることなくノード位置 $\mathcal{C}(v_{next})$ に正確に停止する. 

\subsubsection{制約条件と目的関数}
有効な解（行動計画）は, 以下の衝突回避制約を常に満たす必要がある. 

\begin{equation}
    \forall t \in \mathcal{T},  \forall a_i,  a_j \in \mathcal{A} \ (i \neq j),  \quad \lVert \bm{p}_i(t) - \bm{p}_j(t) \rVert \geq 2r_{\text{safe}}
\end{equation}

本研究では, ノード上での待機中およびエッジ移動中の双方において, この物理的距離制約を厳密に適用する. 
問題の目的は, 全エージェントが衝突制約を満たしながら始点 $s_i$ から終点 $g_i$ へ到達するまでの総所要時間（Makespan）または総移動コスト（Total Travel Time）を最小化することである. 

% =============================================
% 2.2 MAPD
% =============================================
\subsection{Multi-Agent Pickup and Delivery (MAPD)}
Multi-Agent Pickup and Delivery (MAPD) 問題は, MAPF問題に対してタスクの概念を拡張し, 各タスクに「ピックアップ地点」と「デリバリー地点」を定義した問題である. 
従来のMAPFが単純に始点から終点への移動を扱うのに対し, MAPDではエージェントがピックアップ地点で荷物を積載し, デリバリー地点まで運搬するという一連のプロセスを計画する必要がある. \par
さらに, 本研究で扱う \textbf{Lifelong MAPD} の設定では, タスクが時間の経過とともに動的かつ継続的に発生するため, エージェントは一度の配送で終了することなく, 新たなタスクを次々と処理し続けることが求められる. 

\subsubsection{タスクとシステム構成}
本研究におけるMAPD環境は, 前節で定義したMAPFのタプル $\mathcal{M}$ に加え, タスクセット $\Theta$ を導入することで定義される. 
タスクセット $\Theta = \{ \tau_1,  \tau_2,  \dots \}$ は, システムに投入される配送リクエストの集合である. 各タスク $\tau_j \in \Theta$ は以下のタプルで表される. 

\begin{equation}
    \tau_j = \langle v_{j}^{\text{pick}},  v_{j}^{\text{drop}},  t_{j}^{\text{release}} \rangle
\end{equation}

ここで, 各要素は以下の通りである. 
\begin{itemize}
    \item $v_{j}^{\text{pick}} \in V$: 荷物のピックアップ場所(始点). 
    \item $v_{j}^{\text{drop}} \in V$: 荷物の配送場所(終点). 
    \item $t_{j}^{\text{release}} \in \mathcal{T}$: タスクがシステムに発生し, 割り当て可能となる時刻. 
\end{itemize}

\subsubsection{エージェントの状態遷移}
MAPDにおいて, 各エージェント $a_i \in \mathcal{A}$ は, 自身のタスク保持状況に応じて以下の3つの状態のいずれかをとる. 

\begin{enumerate}
    \item \textbf{Free (待機状態)}: タスクを割り当てられていない状態. 
    \item \textbf{ToPick (ピックアップ移動中)}: タスク $\tau_j$ を割り当てられ, 現在位置から $v_{j}^{\text{pick}}$ へ移動している状態. 
    \item \textbf{ToDrop (配送移動中)}: $v_{j}^{\text{pick}}$ に到達して荷物を積載し, 目的地 $v_{j}^{\text{drop}}$ へ移動している状態. 
\end{enumerate}

\subsubsection{MAPFとの関係}
MAPDは, タスク割り当て層と経路計画層の2層構造として捉えることができる. 
各時刻 $t$ において, Free状態のエージェントに対し未割り当てのタスク $\tau_j$ が割り当てられると, そのエージェントの目的地 $g_i$ が更新される. 

\begin{equation}
    g_i = 
    \begin{cases}
        v_{j}^{\text{pick}} & (\text{if state is ToPick}) \\
        v_{j}^{\text{drop}} & (\text{if state is ToDrop}) \\
        \text{nil} & (\text{if state is Free})
    \end{cases}
\end{equation}

このようにして動的に決定される目的地 $g_i$ と, 現在のエージェント位置 $\mathbf{p}_i(t)$ を入力として, 前節で定義したMAPF問題を各タイムステップ(または一定間隔)で解くことにより, エージェントの具体的な行動 $act_{i, t}$ が決定される. 
本研究の目的は, 全てのタスクを完了させるまでの総所要時間, あるいは単位時間あたりのタスク処理数(スループット)を最適化することである. 

% =============================================
% 2. 3 PIBT
% =============================================
\subsection{Priority Inheritance with Backtracking (PIBT)}
Priority Inheritance with Backtracking (PIBT) \cite{Okumura19} は, 各タイムステップにおいてエージェントの次の移動先を決定するための分散型・反応型のアルゴリズムである. 
従来のCBSのような探索ベースの手法が「全エージェントの全経路」を事前に計算するのに対し, PIBTは「現在の1ステップ先の移動」のみを非常に低い計算コストで決定する. そのため, エージェント数が多い大規模環境や, タスクが逐次発生するMAPDのようなオンライン環境において優れたスケーラビリティを発揮する. 
なお, 本節では説明の簡略化のため, 図\ref{fig:two_images}(a)のようなグリッド環境を仮定してPIBTのロジックを解説する. 

\subsubsection{動作原理}
PIBTによる衝突回避は, 主に以下の3つのメカニズムによって実現される. 

\begin{description}
  \item[優先度に基づく意思決定]
  各タイムステップにおいて, 全てのエージェントには何らかの規則に基づいて「優先度」が付与される. PIBTは, 優先度の高いエージェントから順に次の移動先を決定する. 優先度が高いエージェント $a_i$ が隣接ノード $v$ へ移動しようとした際, そのノードが空いている場合は即座に移動が確定する. 

  \item[優先度継承 (Priority Inheritance)]
  PIBTの最大の特徴である. 高優先度のエージェント $a_i$ が移動したいノード $v$ に, 未だ移動先が決定していない低優先度のエージェント $a_j$ が存在する場合, $a_i$ は $a_j$ に対して自身の優先度を一時的に「継承」させる. 
  優先度を受け取った $a_j$ は, 本来の自身の順番を待つことなく, 直ちに $a_i$ のために場所を空けるよう移動先を探索・決定する. これにより, 高優先度のエージェントの進行方向にある障害(他エージェント)が玉突き的に排除され, スムーズな移動が可能となる. 

  \item[バックトラッキング (Backtracking)]
  もし $a_j$ が周囲を他の高優先度エージェントや障害物に囲まれており, 有効な移動先が見つからない場合, 移動の試行は失敗とみなされ, 呼び出し元である $a_i$ に失敗が通知される. これを受け, $a_i$ は別の候補ノードへの移動を試みるか, その場で待機することになる. このプロセスは再帰的に行われる. 
\end{description}

\subsubsection{アルゴリズムの手順}
上述の原理に基づく具体的な手続きは以下の通りである. 各タイムステップ $t$ において, システムは以下の処理を実行する. 

\begin{enumerate}
  \item 全エージェントを優先度の降順にソートする. 
  \item 未計画のエージェントの中で最も優先度の高いものから順に, 関数 \texttt{move($a_i$)} を呼び出す. 
  \item \texttt{move($a_i$)} の内部処理:
  \begin{itemize}
    \item $a_i$ の目的地への距離が短くなるような隣接ノード $v$ を候補とする. 
    \item $v$ が空いている (占有予定がない) 場合, その場所を確保し終了する. 
    \item $v$ に他のエージェント $a_j$ がいる場合:
    \begin{itemize}
        \item もし $a_j$ の移動先が既に決定済みであれば, $v$ への移動は不可とし, 別の候補を探す. 
        \item もし $a_j$ の移動先が未定であれば, $a_j$ に優先度を継承し, 再帰的に \texttt{move($a_j$)} を呼び出す. 
        \item $a_j$ が移動に成功すれば, $a_i$ は $v$ を確保する. 失敗すれば $a_i$ は $v$ への移動を諦め, 次の候補を探すか待機を選択する. 
    \end{itemize}
  \end{itemize}
\end{enumerate}

このアルゴリズムにより, 局所的な衝突を回避しながら, 計算時間をエージェント数に対して線形に近いオーダーに抑えることができる. しかし, PIBTの性能 (スループットや到達率) は「どのエージェントに高い優先度を与えるか」という優先度規則に強く依存する. 単純な規則 (例：ランダム, ID順) では, 複雑なマップ構造においてデッドロックや非効率な動きが発生しやすいため, 本研究ではこの優先度決定の最適化に焦点を当てる. 


% =============================================
% 2.4 進化戦略 (Evolution Strategy)
% =============================================
\subsection{進化戦略 (Evolution Strategy)}
本研究では, PIBTにおけるヒューリスティックな優先度パラメータを最適化するために, 進化戦略 (Evolution Strategy: ES) を用いる. 進化戦略は, 生物の進化の過程に着想を得たブラックボックス最適化手法の一種であり, 関数の勾配情報が得られない, あるいは勾配の計算が困難な問題に対して有効である\cite{Rechenberg73}. 

強化学習 (Reinforcement Learning: RL) がエージェントの行動に対する価値関数や方策勾配を学習するのに対し, 進化戦略はパラメータ空間上で直接探索を行う. Salimansらは, 進化戦略が深層強化学習の代替として競争力のある性能を発揮し, 特に並列化効率において優れていることを示した\cite{Salimans17}. MAPD問題のようなマルチエージェントシミュレーションは, エージェント間の相互作用が複雑であり, 報酬関数が微分不可能であるため, 勾配を用いないESのアプローチは適している. 

% 微分不可能性について
% エージェントの行動は「右に移動」「待機」といった離散的な選択です.  プログラム内部では argmax（最大値を選ぶ操作）や if 文（条件分岐）が多用されます. 

% 例: if 距離 < 安全距離 then 停止 else 移動 このような「条件分岐」や「整数の座標移動」は, 入力パラメータを 0. 0001 だけ変えても出力（行動）が全く変わらない（勾配が 0）か, ある閾値で急激に変わる（勾配が無限大/不連続）ため, 数学的な微分ができません. 

\subsubsection{アルゴリズムの概要}
一般的な進化戦略(Natural Evolution Strategiesの変種)では, パラメータベクトル $\theta$ を中心とした正規分布 $\mathcal{N}(\theta,  \sigma^2 I)$ から, 摂動(ノイズ) $\epsilon_i$ を加えた $N$ 個の候補パラメータ $\theta_i = \theta + \sigma \epsilon_i$ を生成する. ここで $\sigma$ はノイズの標準偏差である. 
各候補パラメータを用いて環境でシミュレーションを行い, 得られた報酬 $F(\theta_i)$ を評価値とする. パラメータの更新は, 高い報酬を得た方向へ分布の中心を移動させることで行われる. 更新則は次式で近似される. 

\begin{equation}
    \theta \leftarrow \theta + \alpha \frac{1}{N \sigma} \sum_{i=1}^{N} F(\theta + \sigma \epsilon_i) \epsilon_i
\end{equation}

ここで, $\alpha$ は学習率を表す. この更新式は, 期待報酬の勾配を有限差分法により確率的に推定していると解釈できる. 

\subsubsection{安定化のための技術}
本研究の実装においては, 学習の安定性と収束速度を向上させるために, 以下の2つの主要な技術を導入している. 

\begin{description}
    \item[対抗変数法 (Antithetic Sampling)]
    勾配推定の分散を低減させるための手法である\cite{Geweke88}. ランダムなノイズ $\epsilon$ を生成する際, $\epsilon$ とその反転である $-\epsilon$ のペアを同時に個体群として生成する. これにより, 正負の摂動に対する対称的なサンプリングが保証され, サンプリングノイズによる推定精度の悪化を抑制する効果がある. 

    \item[適合度シェイピング (Fitness Shaping)]
    得られた報酬 $F(\theta_i)$ をそのまま重みとして使うのではなく, 集団内での順位 (ランク) に基づいて変換する手法である\cite{Wierstra14}. 具体的には, 報酬の値を昇順に並べ替え, [-0.5,  0.5] の範囲などに正規化して利用する．これにより, 極端に大きな報酬値 (外れ値) が勾配推定に与える過度な影響を排除し, 局所解への早期収束を防ぐロバストな学習が可能となる．
\end{description}

本研究では, これらの拡張を施した進化戦略を用いて, PIBTにおける優先度決定のための重みパラメータ (ゴールへの距離, エージェント間の混雑度などの重み係数) を最適化する．
